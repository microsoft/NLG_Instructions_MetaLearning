description: HP Search run for natural instructions Meta
target:
  # service: amlk8s
  # name: itphyperdgx2cl1
  # vc: hai3
  
  # service: amlk8s
  # name: itplabrr1cl1
  # vc: resrchvc
  
  service: amlk8s
  name: itplabrr1cl1
  vc: resrchvc
  
  # # Standard_NC24rs_v2
  # service: amlk8s
  # name: itpeusp100cl
  # vc: resrchvc
#  for available target amlt target info: use --pre for general researchvc clusters
environment:
  # https://phillytools.azurewebsites.net/master/advanced/5_customizing_dockers.html
  # image: huggingface/transformers-pytorch-gpu:latest
  # registry: registry.hub.docker.com   # this is the default
  image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-devel
  # image: pytorch/pytorch:1.4-cuda10.1-cudnn7-devel
  # registry: docker.io # any public registry can be specified here
  image_setup:
  - pip install transformers==4.11.0
  - pip install datasets
  - pip install ruamel.yaml==0.16 --disable-pip-version-check
  - pip install rouge_score
  - pip install accelerate
  - pip install py7zr
  - pip install rouge-metric
  - pip install learn2learn
  - pip install higher

storage:
  data:
    storage_account_name: budebsmartreplymltl
    container_name: amulet
    mount_dir: /mnt/amulet

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: /mnt/Repos/LIT/Projects/NLGNI/

data:
  local_dir: /mnt/Data/natural_instructions_v2.5/
  remote_dir: Data

 

search:
  job_template:
    name: HPsearch_{experiment_name:s}_{add_instruction}_{add_positive}_{add_explanation}_{num_examples}
    sku: G1
    command:
    - python $$AMLT_CODE_DIR/data/natural_instructions_dataset/train.py
      --output_dir $$AMLT_DATA_DIR/models/natural_instructions_v2.5/itplabrr1cl1/HNET/BBase_10_{add_instruction}_{add_positive}_{add_explanation}_{train_task_list}_{lr}_{num_train_instances}_{ga_steps}_{wa_steps}_{hnet_exc_enc_or_dec}_{hnet_opt}_{hnet_hidden_dim}_{hnet_nograd}_{hnet_alt_steps}_{last_hidden_state}_6_19_22
      --model_name_or_path {model_name} --tokenizer_name {model_name}
      --dataset_name natural_instructions_v2.5 --dataset_folder $$AMLT_DATA_DIR/natural_instructions_v2.5/ --overwrite_output_dir True
      --cache_dir $$AMLT_DATA_DIR/model_cache_pt/ --overwrite_cache True
      --do_train --do_eval --do_predict --predict_with_generate
      --max_source_length 1024 --max_target_length 128 --decode_max_length 128
      --metric_for_best_model rouge-l-f --multi_ref_mode best
      --disable_tqdm True --save_checkpoints True --resume_from_checkpoint True
      --num_train_epochs {epochs} --eval_steps {eval_steps}  --logging_steps 50  --save_steps {eval_steps} 
      --load_best_model_at_end True 
      --evaluation_strategy steps --metric_for_best_model rouge-l-f --multi_ref_mode best
      --log_task_level_metrics True --show_example_predictions 0 --log_level warning
      --use_train_tasks_list {train_task_list} --use_exclude_tasks_list {exclude_task_list}
      --use_test_tasks_list {test_task_list} --use_eval_tasks_list {eval_task_list}
      --train_test_split_mode random_tasks_k_shot --filter_dataset_by_length True
      --num_kshot_train_instances_per_task {num_kshot_train_instances}
      --num_train_instances_per_task {num_train_instances} --num_test_instances_per_task 50 --num_eval_instances_per_task 100
      --adafactor True --fp16 False --lr_scheduler_type linear --gradient_checkpointing False
      --learning_rate {lr} --warmup_steps {wa_steps} --gradient_accumulation_steps {ga_steps} --lr_scheduler_type linear
      --add_category False --add_task_summary False --add_instruction {add_instruction} --add_positive_examples {add_positive}
      --add_negative_examples False --add_explanations {add_explanation}  --num_examples_in_instruction {num_examples} --prepend_inst_in_enc_or_dec encoder
      --filter_sequences_by_len_diff_from_ref False --pred_ref_diff_tolerance 1.0
      --train_loop_opt_type hnet 
      --hnet_model_name_or_path {model_name} --hnet_tokenizer_name {model_name}  
      --hnet_add_instruction_to_mainlm True --hnet_use_encoder_last_hidden_state {last_hidden_state}  
      --hnet_exclude_encoder_or_decoder {hnet_exc_enc_or_dec}
      --hnet_opt_mode {hnet_opt} --hnet_hidden_dim {hnet_hidden_dim} --hnet_alternating_num_steps {hnet_alt_steps}
      --hnet_use_eval_for_nograd {hnet_nograd}
      --hnet_initial_alt_mode {init_alt_mode}
      --use_train_dataset_for_eval True
      --run_hnet_in_batch_mode_per_task True --task_batch_size_per_iter 10  --num_tasks_per_iter 1
      --per_device_train_batch_size 1 --per_device_eval_batch_size 6   

  max_trials: 16
  parallel_trials: 16
  max_duration_hours: 120
  metrics: # optimization objective. Required for bayesian sampling and early_termination, ignored otherwise
    - name: eval_rouge-l-f
      goal: maximize
  sampling: grid
  params:
    - name: model_name
      values: choice("facebook/bart-base")
    - name: eval_steps
      values: choice(1000)
    - name: epochs
      values: choice(10)
    - name: num_train_instances
      values: choice(100)
    - name: ga_steps
      values: choice(10)
    - name: hnet_opt
      values: choice("alternating_hnet_main")
    - name: hnet_exc_enc_or_dec
      values: choice("restricted_encoder")
    - name: hnet_hidden_dim
      values: choice(256)
    - name: hnet_alt_steps
      values: choice(10, 20)
    - name: hnet_nograd
      values: choice("True")
    - name: init_alt_mode
      values: choice("hnet")
    - name: last_hidden_state
      values: choice("False")
    - name: lr
      values: choice(0.00001)
    - name: wa_steps
      values: choice(500)
    - name: num_kshot_train_instances
      values: choice(0)
    - name: add_instruction
      values: choice("True", "False")
    - name: add_positive
      values: choice("True", "False")
    - name: add_explanation
      values: choice("False")
    - name: num_examples
      values: choice(1)
    - name: train_task_list
      values: choice("False")
    - name: test_task_list
      values: choice("False")
    - name: eval_task_list
      values: choice("TEST_TASKS_LIST_SUMM_TITLE")
    - name: exclude_task_list
      values: choice("EXCLUDE_TASKS_LIST_SUMM_TITLE")